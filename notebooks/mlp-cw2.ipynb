{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:17:43.903226Z",
     "iopub.status.busy": "2021-12-03T15:17:43.902921Z",
     "iopub.status.idle": "2021-12-03T15:17:45.582624Z",
     "shell.execute_reply": "2021-12-03T15:17:45.581970Z",
     "shell.execute_reply.started": "2021-12-03T15:17:43.903143Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:17:47.734056Z",
     "iopub.status.busy": "2021-12-03T15:17:47.733398Z",
     "iopub.status.idle": "2021-12-03T15:17:47.742010Z",
     "shell.execute_reply": "2021-12-03T15:17:47.741274Z",
     "shell.execute_reply.started": "2021-12-03T15:17:47.734016Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.listdir('/kaggle/working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:22:23.428391Z",
     "iopub.status.busy": "2021-12-03T15:22:23.427615Z",
     "iopub.status.idle": "2021-12-03T15:22:30.062293Z",
     "shell.execute_reply": "2021-12-03T15:22:30.061240Z",
     "shell.execute_reply.started": "2021-12-03T15:22:23.428348Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/experiments\n",
    "!mkdir /kaggle/working/experiments/bn\n",
    "!mkdir /kaggle/working/experiments/bnrc\n",
    "!mkdir /kaggle/working/experiments/vanilla\n",
    "!mkdir /kaggle/working/experiments/vanilla/result_outputs\n",
    "!mkdir /kaggle/working/experiments/vanilla/saved_models\n",
    "!mkdir /kaggle/working/experiments/bn/result_outputs\n",
    "!mkdir /kaggle/working/experiments/bn/saved_models\n",
    "!mkdir /kaggle/working/experiments/bnrc/result_outputs\n",
    "!mkdir /kaggle/working/experiments/bnrc/saved_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:18:03.140235Z",
     "iopub.status.busy": "2021-12-03T15:18:03.139597Z",
     "iopub.status.idle": "2021-12-03T15:18:03.146088Z",
     "shell.execute_reply": "2021-12-03T15:18:03.145350Z",
     "shell.execute_reply.started": "2021-12-03T15:18:03.140194Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    batch_size = 264\n",
    "    image_height = 32\n",
    "    image_width = 32\n",
    "    num_channels = 3\n",
    "    num_classes = 100\n",
    "    num_filters = 16\n",
    "    num_epochs = 250\n",
    "    num_stages = 3\n",
    "    num_blocks_per_stage = 5\n",
    "    lr_bn = 1e-3\n",
    "    lr_bn_rc = 1e-2\n",
    "    weight_decay = 1e-6\n",
    "    input_shape = (batch_size,num_channels,image_height,image_width)\n",
    "    criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:18:06.518174Z",
     "iopub.status.busy": "2021-12-03T15:18:06.517894Z",
     "iopub.status.idle": "2021-12-03T15:18:06.534356Z",
     "shell.execute_reply": "2021-12-03T15:18:06.533624Z",
     "shell.execute_reply.started": "2021-12-03T15:18:06.518143Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def save_to_stats_pkl_file(experiment_log_filepath, filename, stats_dict):\n",
    "    summary_filename = os.path.join(experiment_log_filepath, filename)\n",
    "    with open(\"{}.pkl\".format(summary_filename), \"wb\") as file_writer:\n",
    "        pickle.dump(stats_dict, file_writer)\n",
    "\n",
    "\n",
    "def load_from_stats_pkl_file(experiment_log_filepath, filename):\n",
    "    summary_filename = os.path.join(experiment_log_filepath, filename)\n",
    "    with open(\"{}.pkl\".format(summary_filename), \"rb\") as file_reader:\n",
    "        stats = pickle.load(file_reader)\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def save_statistics(experiment_log_dir, filename, stats_dict, current_epoch, continue_from_mode=False, save_full_dict=False):\n",
    "    \"\"\"\n",
    "    Saves the statistics in stats dict into a csv file. Using the keys as the header entries and the values as the\n",
    "    columns of a particular header entry\n",
    "    :param experiment_log_dir: the log folder dir filepath\n",
    "    :param filename: the name of the csv file\n",
    "    :param stats_dict: the stats dict containing the data to be saved\n",
    "    :param current_epoch: the number of epochs since commencement of the current training session (i.e. if the experiment continued from 100 and this is epoch 105, then pass relative distance of 5.)\n",
    "    :param save_full_dict: whether to save the full dict as is overriding any previous entries (might be useful if we want to overwrite a file)\n",
    "    :return: The filepath to the summary file\n",
    "    \"\"\"\n",
    "    summary_filename = os.path.join(experiment_log_dir, filename)\n",
    "    mode = 'a' if continue_from_mode else 'w'\n",
    "    with open(summary_filename, mode) as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not continue_from_mode:\n",
    "            writer.writerow(list(stats_dict.keys()))\n",
    "\n",
    "        if save_full_dict:\n",
    "            total_rows = len(list(stats_dict.values())[0])\n",
    "            for idx in range(total_rows):\n",
    "                row_to_add = [value[idx] for value in list(stats_dict.values())]\n",
    "                writer.writerow(row_to_add)\n",
    "        else:\n",
    "            row_to_add = [value[current_epoch] for value in list(stats_dict.values())]\n",
    "            writer.writerow(row_to_add)\n",
    "\n",
    "    return summary_filename\n",
    "\n",
    "\n",
    "def load_statistics(experiment_log_dir, filename):\n",
    "    \"\"\"\n",
    "    Loads a statistics csv file into a dictionary\n",
    "    :param experiment_log_dir: the log folder dir filepath\n",
    "    :param filename: the name of the csv file to load\n",
    "    :return: A dictionary containing the stats in the csv file. Header entries are converted into keys and columns of a\n",
    "     particular header are converted into values of a key in a list format.\n",
    "    \"\"\"\n",
    "    summary_filename = os.path.join(experiment_log_dir, filename)\n",
    "\n",
    "    with open(summary_filename, 'r+') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    keys = lines[0].split(\",\")\n",
    "    stats = {key: [] for key in keys}\n",
    "    for line in lines[1:]:\n",
    "        values = line.split(\",\")\n",
    "        for idx, value in enumerate(values):\n",
    "            stats[keys[idx]].append(value)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:18:11.479938Z",
     "iopub.status.busy": "2021-12-03T15:18:11.479677Z",
     "iopub.status.idle": "2021-12-03T15:18:11.582298Z",
     "shell.execute_reply": "2021-12-03T15:18:11.581629Z",
     "shell.execute_reply.started": "2021-12-03T15:18:11.479908Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Data providers.\n",
    "This module provides classes for loading datasets and iterating over batches of\n",
    "data points.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.utils import download_url, check_integrity\n",
    "\n",
    "#from mlp import DEFAULT_SEED\n",
    "DEFAULT_SEED = 123456\n",
    "\n",
    "class DataProvider(object):\n",
    "    \"\"\"Generic data provider.\"\"\"\n",
    "\n",
    "    def __init__(self, inputs, targets, batch_size, max_num_batches=-1,\n",
    "                 shuffle_order=True, rng=None):\n",
    "        \"\"\"Create a new data provider object.\n",
    "        Args:\n",
    "            inputs (ndarray): Array of data input features of shape\n",
    "                (num_data, input_dim).\n",
    "            targets (ndarray): Array of data output targets of shape\n",
    "                (num_data, output_dim) or (num_data,) if output_dim == 1.\n",
    "            batch_size (int): Number of data points to include in each batch.\n",
    "            max_num_batches (int): Maximum number of batches to iterate over\n",
    "                in an epoch. If `max_num_batches * batch_size > num_data` then\n",
    "                only as many batches as the data can be split into will be\n",
    "                used. If set to -1 all of the data will be used.\n",
    "            shuffle_order (bool): Whether to randomly permute the order of\n",
    "                the data before each epoch.\n",
    "            rng (RandomState): A seeded random number generator.\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        if batch_size < 1:\n",
    "            raise ValueError('batch_size must be >= 1')\n",
    "        self._batch_size = batch_size\n",
    "        if max_num_batches == 0 or max_num_batches < -1:\n",
    "            raise ValueError('max_num_batches must be -1 or > 0')\n",
    "        self._max_num_batches = max_num_batches\n",
    "        self._update_num_batches()\n",
    "        self.shuffle_order = shuffle_order\n",
    "        self._current_order = np.arange(inputs.shape[0])\n",
    "        if rng is None:\n",
    "            rng = np.random.RandomState(DEFAULT_SEED)\n",
    "        self.rng = rng\n",
    "        self.new_epoch()\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        \"\"\"Number of data points to include in each batch.\"\"\"\n",
    "        return self._batch_size\n",
    "\n",
    "    @batch_size.setter\n",
    "    def batch_size(self, value):\n",
    "        if value < 1:\n",
    "            raise ValueError('batch_size must be >= 1')\n",
    "        self._batch_size = value\n",
    "        self._update_num_batches()\n",
    "\n",
    "    @property\n",
    "    def max_num_batches(self):\n",
    "        \"\"\"Maximum number of batches to iterate over in an epoch.\"\"\"\n",
    "        return self._max_num_batches\n",
    "\n",
    "    @max_num_batches.setter\n",
    "    def max_num_batches(self, value):\n",
    "        if value == 0 or value < -1:\n",
    "            raise ValueError('max_num_batches must be -1 or > 0')\n",
    "        self._max_num_batches = value\n",
    "        self._update_num_batches()\n",
    "\n",
    "    def _update_num_batches(self):\n",
    "        \"\"\"Updates number of batches to iterate over.\"\"\"\n",
    "        # maximum possible number of batches is equal to number of whole times\n",
    "        # batch_size divides in to the number of data points which can be\n",
    "        # found using integer division\n",
    "        possible_num_batches = self.inputs.shape[0] // self.batch_size\n",
    "        if self.max_num_batches == -1:\n",
    "            self.num_batches = possible_num_batches\n",
    "        else:\n",
    "            self.num_batches = min(self.max_num_batches, possible_num_batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Implements Python iterator interface.\n",
    "        This should return an object implementing a `next` method which steps\n",
    "        through a sequence returning one element at a time and raising\n",
    "        `StopIteration` when at the end of the sequence. Here the object\n",
    "        returned is the DataProvider itself.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def new_epoch(self):\n",
    "        \"\"\"Starts a new epoch (pass through data), possibly shuffling first.\"\"\"\n",
    "        self._curr_batch = 0\n",
    "        if self.shuffle_order:\n",
    "            self.shuffle()\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the provider to the initial state.\"\"\"\n",
    "        inv_perm = np.argsort(self._current_order)\n",
    "        self._current_order = self._current_order[inv_perm]\n",
    "        self.inputs = self.inputs[inv_perm]\n",
    "        self.targets = self.targets[inv_perm]\n",
    "        self.new_epoch()\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"Randomly shuffles order of data.\"\"\"\n",
    "        perm = self.rng.permutation(self.inputs.shape[0])\n",
    "        self._current_order = self._current_order[perm]\n",
    "        self.inputs = self.inputs[perm]\n",
    "        self.targets = self.targets[perm]\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Returns next data batch or raises `StopIteration` if at end.\"\"\"\n",
    "        if self._curr_batch + 1 > self.num_batches:\n",
    "            # no more batches in current iteration through data set so start\n",
    "            # new epoch ready for another pass and indicate iteration is at end\n",
    "            self.new_epoch()\n",
    "            raise StopIteration()\n",
    "        # create an index slice corresponding to current batch number\n",
    "        batch_slice = slice(self._curr_batch * self.batch_size,\n",
    "                            (self._curr_batch + 1) * self.batch_size)\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        targets_batch = self.targets[batch_slice]\n",
    "        self._curr_batch += 1\n",
    "        return inputs_batch, targets_batch\n",
    "\n",
    "class MNISTDataProvider(DataProvider):\n",
    "    \"\"\"Data provider for MNIST handwritten digit images.\"\"\"\n",
    "\n",
    "    def __init__(self, which_set='train', batch_size=100, max_num_batches=-1,\n",
    "                 shuffle_order=True, rng=None):\n",
    "        \"\"\"Create a new MNIST data provider object.\n",
    "        Args:\n",
    "            which_set: One of 'train', 'valid' or 'eval'. Determines which\n",
    "                portion of the MNIST data this object should provide.\n",
    "            batch_size (int): Number of data points to include in each batch.\n",
    "            max_num_batches (int): Maximum number of batches to iterate over\n",
    "                in an epoch. If `max_num_batches * batch_size > num_data` then\n",
    "                only as many batches as the data can be split into will be\n",
    "                used. If set to -1 all of the data will be used.\n",
    "            shuffle_order (bool): Whether to randomly permute the order of\n",
    "                the data before each epoch.\n",
    "            rng (RandomState): A seeded random number generator.\n",
    "        \"\"\"\n",
    "        # check a valid which_set was provided\n",
    "        assert which_set in ['train', 'valid', 'test'], (\n",
    "            'Expected which_set to be either train, valid or eval. '\n",
    "            'Got {0}'.format(which_set)\n",
    "        )\n",
    "        self.which_set = which_set\n",
    "        self.num_classes = 10\n",
    "        # construct path to data using os.path.join to ensure the correct path\n",
    "        # separator for the current platform / OS is used\n",
    "        # MLP_DATA_DIR environment variable should point to the data directory\n",
    "        data_path = os.path.join(\n",
    "            os.environ['MLP_DATA_DIR'], 'mnist-{0}.npz'.format(which_set))\n",
    "        assert os.path.isfile(data_path), (\n",
    "            'Data file does not exist at expected path: ' + data_path\n",
    "        )\n",
    "        # load data from compressed numpy file\n",
    "        loaded = np.load(data_path)\n",
    "        inputs, targets = loaded['inputs'], loaded['targets']\n",
    "        inputs = inputs.astype(np.float32)\n",
    "        # pass the loaded data to the parent class __init__\n",
    "        super(MNISTDataProvider, self).__init__(\n",
    "            inputs, targets, batch_size, max_num_batches, shuffle_order, rng)\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Returns next data batch or raises `StopIteration` if at end.\"\"\"\n",
    "        inputs_batch, targets_batch = super(MNISTDataProvider, self).next()\n",
    "        return inputs_batch, self.to_one_of_k(targets_batch)\n",
    "\n",
    "    def to_one_of_k(self, int_targets):\n",
    "        \"\"\"Converts integer coded class target to 1 of K coded targets.\n",
    "        Args:\n",
    "            int_targets (ndarray): Array of integer coded class targets (i.e.\n",
    "                where an integer from 0 to `num_classes` - 1 is used to\n",
    "                indicate which is the correct class). This should be of shape\n",
    "                (num_data,).\n",
    "        Returns:\n",
    "            Array of 1 of K coded targets i.e. an array of shape\n",
    "            (num_data, num_classes) where for each row all elements are equal\n",
    "            to zero except for the column corresponding to the correct class\n",
    "            which is equal to one.\n",
    "        \"\"\"\n",
    "        one_of_k_targets = np.zeros((int_targets.shape[0], self.num_classes))\n",
    "        one_of_k_targets[range(int_targets.shape[0]), int_targets] = 1\n",
    "        return one_of_k_targets\n",
    "\n",
    "class EMNISTDataProvider(DataProvider):\n",
    "    \"\"\"Data provider for EMNIST handwritten digit images.\"\"\"\n",
    "\n",
    "    def __init__(self, which_set='train', batch_size=100, max_num_batches=-1,\n",
    "                 shuffle_order=True, rng=None, flatten=False):\n",
    "        \"\"\"Create a new EMNIST data provider object.\n",
    "        Args:\n",
    "            which_set: One of 'train', 'valid' or 'eval'. Determines which\n",
    "                portion of the EMNIST data this object should provide.\n",
    "            batch_size (int): Number of data points to include in each batch.\n",
    "            max_num_batches (int): Maximum number of batches to iterate over\n",
    "                in an epoch. If `max_num_batches * batch_size > num_data` then\n",
    "                only as many batches as the data can be split into will be\n",
    "                used. If set to -1 all of the data will be used.\n",
    "            shuffle_order (bool): Whether to randomly permute the order of\n",
    "                the data before each epoch.\n",
    "            rng (RandomState): A seeded random number generator.\n",
    "        \"\"\"\n",
    "        # check a valid which_set was provided\n",
    "        assert which_set in ['train', 'valid', 'test'], (\n",
    "            'Expected which_set to be either train, valid or eval. '\n",
    "            'Got {0}'.format(which_set)\n",
    "        )\n",
    "        self.which_set = which_set\n",
    "        self.num_classes = 47\n",
    "        # construct path to data using os.path.join to ensure the correct path\n",
    "        # separator for the current platform / OS is used\n",
    "        # MLP_DATA_DIR environment variable should point to the data directory\n",
    "        data_path = os.path.join(\n",
    "            os.environ['MLP_DATA_DIR'], 'emnist-{0}.npz'.format(which_set))\n",
    "        assert os.path.isfile(data_path), (\n",
    "            'Data file does not exist at expected path: ' + data_path\n",
    "        )\n",
    "        # load data from compressed numpy file\n",
    "        loaded = np.load(data_path)\n",
    "        print(loaded.keys())\n",
    "        inputs, targets = loaded['inputs'], loaded['targets']\n",
    "        inputs = inputs.astype(np.float32)\n",
    "        targets = targets.astype(np.int)\n",
    "        if flatten:\n",
    "            inputs = np.reshape(inputs, newshape=(-1, 28*28))\n",
    "        else:\n",
    "            inputs = np.reshape(inputs, newshape=(-1, 28, 28, 1))\n",
    "        inputs = inputs / 255.0\n",
    "        # pass the loaded data to the parent class __init__\n",
    "        super(EMNISTDataProvider, self).__init__(\n",
    "            inputs, targets, batch_size, max_num_batches, shuffle_order, rng)\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Returns next data batch or raises `StopIteration` if at end.\"\"\"\n",
    "        inputs_batch, targets_batch = super(EMNISTDataProvider, self).next()\n",
    "        return inputs_batch, self.to_one_of_k(targets_batch)\n",
    "\n",
    "    def to_one_of_k(self, int_targets):\n",
    "        \"\"\"Converts integer coded class target to 1 of K coded targets.\n",
    "        Args:\n",
    "            int_targets (ndarray): Array of integer coded class targets (i.e.\n",
    "                where an integer from 0 to `num_classes` - 1 is used to\n",
    "                indicate which is the correct class). This should be of shape\n",
    "                (num_data,).\n",
    "        Returns:\n",
    "            Array of 1 of K coded targets i.e. an array of shape\n",
    "            (num_data, num_classes) where for each row all elements are equal\n",
    "            to zero except for the column corresponding to the correct class\n",
    "            which is equal to one.\n",
    "        \"\"\"\n",
    "        one_of_k_targets = np.zeros((int_targets.shape[0], self.num_classes))\n",
    "        one_of_k_targets[range(int_targets.shape[0]), int_targets] = 1\n",
    "        return one_of_k_targets\n",
    "\n",
    "class MetOfficeDataProvider(DataProvider):\n",
    "    \"\"\"South Scotland Met Office weather data provider.\"\"\"\n",
    "\n",
    "    def __init__(self, window_size, batch_size=10, max_num_batches=-1,\n",
    "                 shuffle_order=True, rng=None):\n",
    "        \"\"\"Create a new Met Office data provider object.\n",
    "        Args:\n",
    "            window_size (int): Size of windows to split weather time series\n",
    "               data into. The constructed input features will be the first\n",
    "               `window_size - 1` entries in each window and the target outputs\n",
    "               the last entry in each window.\n",
    "            batch_size (int): Number of data points to include in each batch.\n",
    "            max_num_batches (int): Maximum number of batches to iterate over\n",
    "                in an epoch. If `max_num_batches * batch_size > num_data` then\n",
    "                only as many batches as the data can be split into will be\n",
    "                used. If set to -1 all of the data will be used.\n",
    "            shuffle_order (bool): Whether to randomly permute the order of\n",
    "                the data before each epoch.\n",
    "            rng (RandomState): A seeded random number generator.\n",
    "        \"\"\"\n",
    "        data_path = os.path.join(\n",
    "            os.environ['MLP_DATA_DIR'], 'HadSSP_daily_qc.txt')\n",
    "        assert os.path.isfile(data_path), (\n",
    "            'Data file does not exist at expected path: ' + data_path\n",
    "        )\n",
    "        raw = np.loadtxt(data_path, skiprows=3, usecols=range(2, 32))\n",
    "        assert window_size > 1, 'window_size must be at least 2.'\n",
    "        self.window_size = window_size\n",
    "        # filter out all missing datapoints and flatten to a vector\n",
    "        filtered = raw[raw >= 0].flatten()\n",
    "        # normalise data to zero mean, unit standard deviation\n",
    "        mean = np.mean(filtered)\n",
    "        std = np.std(filtered)\n",
    "        normalised = (filtered - mean) / std\n",
    "        # create a view on to array corresponding to a rolling window\n",
    "        shape = (normalised.shape[-1] - self.window_size + 1, self.window_size)\n",
    "        strides = normalised.strides + (normalised.strides[-1],)\n",
    "        windowed = np.lib.stride_tricks.as_strided(\n",
    "            normalised, shape=shape, strides=strides)\n",
    "        # inputs are first (window_size - 1) entries in windows\n",
    "        inputs = windowed[:, :-1]\n",
    "        # targets are last entry in windows\n",
    "        targets = windowed[:, -1]\n",
    "        super(MetOfficeDataProvider, self).__init__(\n",
    "            inputs, targets, batch_size, max_num_batches, shuffle_order, rng)\n",
    "\n",
    "class CCPPDataProvider(DataProvider):\n",
    "\n",
    "    def __init__(self, which_set='train', input_dims=None, batch_size=10,\n",
    "                 max_num_batches=-1, shuffle_order=True, rng=None):\n",
    "        \"\"\"Create a new Combined Cycle Power Plant data provider object.\n",
    "        Args:\n",
    "            which_set: One of 'train' or 'valid'. Determines which portion of\n",
    "                data this object should provide.\n",
    "            input_dims: Which of the four input dimension to use. If `None` all\n",
    "                are used. If an iterable of integers are provided (consisting\n",
    "                of a subset of {0, 1, 2, 3}) then only the corresponding\n",
    "                input dimensions are included.\n",
    "            batch_size (int): Number of data points to include in each batch.\n",
    "            max_num_batches (int): Maximum number of batches to iterate over\n",
    "                in an epoch. If `max_num_batches * batch_size > num_data` then\n",
    "                only as many batches as the data can be split into will be\n",
    "                used. If set to -1 all of the data will be used.\n",
    "            shuffle_order (bool): Whether to randomly permute the order of\n",
    "                the data before each epoch.\n",
    "            rng (RandomState): A seeded random number generator.\n",
    "        \"\"\"\n",
    "        data_path = os.path.join(\n",
    "            os.environ['MLP_DATA_DIR'], 'ccpp_data.npz')\n",
    "        assert os.path.isfile(data_path), (\n",
    "            'Data file does not exist at expected path: ' + data_path\n",
    "        )\n",
    "        # check a valid which_set was provided\n",
    "        assert which_set in ['train', 'valid'], (\n",
    "            'Expected which_set to be either train or valid '\n",
    "            'Got {0}'.format(which_set)\n",
    "        )\n",
    "        # check input_dims are valid\n",
    "        if not input_dims is not None:\n",
    "            input_dims = set(input_dims)\n",
    "            assert input_dims.issubset({0, 1, 2, 3}), (\n",
    "                'input_dims should be a subset of {0, 1, 2, 3}'\n",
    "            )\n",
    "        loaded = np.load(data_path)\n",
    "        inputs = loaded[which_set + '_inputs']\n",
    "        if input_dims is not None:\n",
    "            inputs = inputs[:, input_dims]\n",
    "        targets = loaded[which_set + '_targets']\n",
    "        super(CCPPDataProvider, self).__init__(\n",
    "            inputs, targets, batch_size, max_num_batches, shuffle_order, rng)\n",
    "\n",
    "class EMNISTPytorchDataProvider(Dataset):\n",
    "    def __init__(self, which_set='train', batch_size=100, max_num_batches=-1,\n",
    "                 shuffle_order=True, rng=None, flatten=False, transforms=None):\n",
    "        self.numpy_data_provider = EMNISTDataProvider(which_set=which_set, batch_size=batch_size, max_num_batches=max_num_batches,\n",
    "                 shuffle_order=shuffle_order, rng=rng, flatten=flatten)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        x = self.numpy_data_provider.inputs[item]\n",
    "        for augmentation in self.transforms:\n",
    "            x = augmentation(x)\n",
    "        return x, int(self.numpy_data_provider.targets[item])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numpy_data_provider.targets)\n",
    "\n",
    "class AugmentedMNISTDataProvider(MNISTDataProvider):\n",
    "    \"\"\"Data provider for MNIST dataset which randomly transforms images.\"\"\"\n",
    "\n",
    "    def __init__(self, which_set='train', batch_size=100, max_num_batches=-1,\n",
    "                 shuffle_order=True, rng=None, transformer=None):\n",
    "        \"\"\"Create a new augmented MNIST data provider object.\n",
    "        Args:\n",
    "            which_set: One of 'train', 'valid' or 'test'. Determines which\n",
    "                portion of the MNIST data this object should provide.\n",
    "            batch_size (int): Number of data points to include in each batch.\n",
    "            max_num_batches (int): Maximum number of batches to iterate over\n",
    "                in an epoch. If `max_num_batches * batch_size > num_data` then\n",
    "                only as many batches as the data can be split into will be\n",
    "                used. If set to -1 all of the data will be used.\n",
    "            shuffle_order (bool): Whether to randomly permute the order of\n",
    "                the data before each epoch.\n",
    "            rng (RandomState): A seeded random number generator.\n",
    "            transformer: Function which takes an `inputs` array of shape\n",
    "                (batch_size, input_dim) corresponding to a batch of input\n",
    "                images and a `rng` random number generator object (i.e. a\n",
    "                call signature `transformer(inputs, rng)`) and applies a\n",
    "                potentiall random set of transformations to some / all of the\n",
    "                input images as each new batch is returned when iterating over\n",
    "                the data provider.\n",
    "        \"\"\"\n",
    "        super(AugmentedMNISTDataProvider, self).__init__(\n",
    "            which_set, batch_size, max_num_batches, shuffle_order, rng)\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Returns next data batch or raises `StopIteration` if at end.\"\"\"\n",
    "        inputs_batch, targets_batch = super(\n",
    "            AugmentedMNISTDataProvider, self).next()\n",
    "        transformed_inputs_batch = self.transformer(inputs_batch, self.rng)\n",
    "        return transformed_inputs_batch, targets_batch\n",
    "\n",
    "class Omniglot(data.Dataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
    "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
    "            creates from test set.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    def collect_data_paths(self, root):\n",
    "        data_dict = dict()\n",
    "        print(root)\n",
    "        for subdir, dir, files in os.walk(root):\n",
    "            for file in files:\n",
    "                if file.endswith('.png'):\n",
    "                    filepath = os.path.join(subdir, file)\n",
    "                    class_label = '_'.join(subdir.split(\"/\")[-2:])\n",
    "                    if class_label in data_dict:\n",
    "                        data_dict[class_label].append(filepath)\n",
    "                    else:\n",
    "                        data_dict[class_label] = [filepath]\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "    def __init__(self, root, set_name,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.root = os.path.abspath(os.path.join(self.root, 'omniglot_dataset'))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.set_name = set_name  # training set or test set\n",
    "        self.data_dict = self.collect_data_paths(root=self.root)\n",
    "\n",
    "        x = []\n",
    "        label_to_idx = {label: idx for idx, label in enumerate(self.data_dict.keys())}\n",
    "        y = []\n",
    "\n",
    "        for key, value in self.data_dict.items():\n",
    "            x.extend(value)\n",
    "            y.extend(len(value) * [label_to_idx[key]])\n",
    "\n",
    "        y = np.array(y)\n",
    "\n",
    "\n",
    "        rng = np.random.RandomState(seed=0)\n",
    "\n",
    "        idx = np.arange(len(x))\n",
    "        rng.shuffle(idx)\n",
    "\n",
    "        x = [x[current_idx] for current_idx in idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        train_sample_idx = rng.choice(a=[i for i in range(len(x))], size=int(len(x) * 0.80), replace=False)\n",
    "        evaluation_sample_idx = [i for i in range(len(x)) if i not in train_sample_idx]\n",
    "        validation_sample_idx = rng.choice(a=[i for i in range(len(evaluation_sample_idx))], size=int(len(evaluation_sample_idx) * 0.40), replace=False)\n",
    "        test_sample_idx = [i for i in range(len(evaluation_sample_idx)) if i not in evaluation_sample_idx]\n",
    "\n",
    "        if self.set_name is 'train':\n",
    "            self.data = [item for idx, item in enumerate(x) if idx in train_sample_idx]\n",
    "            self.labels = y[train_sample_idx]\n",
    "\n",
    "        elif self.set_name is 'val':\n",
    "            self.data = [item for idx, item in enumerate(x) if idx in validation_sample_idx]\n",
    "            self.labels = y[validation_sample_idx]\n",
    "\n",
    "        else:\n",
    "            self.data = [item for idx, item in enumerate(x) if idx in test_sample_idx]\n",
    "            self.labels = y[test_sample_idx]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.labels[index]\n",
    "\n",
    "        img = Image.open(img)\n",
    "        img.show()\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = self.set_name\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "class CIFAR10(data.Dataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
    "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
    "            creates from test set.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "\n",
    "    def __init__(self, root, set_name,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.set_name = set_name  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        rng = np.random.RandomState(seed=0)\n",
    "\n",
    "        train_sample_idx = rng.choice(a=[i for i in range(50000)], size=47500, replace=False)\n",
    "        val_sample_idx = [i for i in range(50000) if i not in train_sample_idx]\n",
    "\n",
    "        if self.set_name is 'train':\n",
    "            self.data = []\n",
    "            self.labels = []\n",
    "            for fentry in self.train_list:\n",
    "                f = fentry[0]\n",
    "                file = os.path.join(self.root, self.base_folder, f)\n",
    "                fo = open(file, 'rb')\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(fo)\n",
    "                else:\n",
    "                    entry = pickle.load(fo, encoding='latin1')\n",
    "                self.data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.labels += entry['labels']\n",
    "                else:\n",
    "                    self.labels += entry['fine_labels']\n",
    "                fo.close()\n",
    "\n",
    "            self.data = np.concatenate(self.data)\n",
    "\n",
    "            self.data = self.data.reshape((50000, 3, 32, 32))\n",
    "            self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "            self.data = self.data[train_sample_idx]\n",
    "            self.labels = np.array(self.labels)[train_sample_idx]\n",
    "            print(set_name, self.data.shape)\n",
    "            print(set_name, self.labels.shape)\n",
    "\n",
    "        elif self.set_name is 'val':\n",
    "            self.data = []\n",
    "            self.labels = []\n",
    "            for fentry in self.train_list:\n",
    "                f = fentry[0]\n",
    "                file = os.path.join(self.root, self.base_folder, f)\n",
    "                fo = open(file, 'rb')\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(fo)\n",
    "                else:\n",
    "                    entry = pickle.load(fo, encoding='latin1')\n",
    "                self.data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.labels += entry['labels']\n",
    "                else:\n",
    "                    self.labels += entry['fine_labels']\n",
    "                fo.close()\n",
    "\n",
    "            self.data = np.concatenate(self.data)\n",
    "            self.data = self.data.reshape((50000, 3, 32, 32))\n",
    "            self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "            self.data = self.data[val_sample_idx]\n",
    "            self.labels = np.array(self.labels)[val_sample_idx]\n",
    "            print(set_name, self.data.shape)\n",
    "            print(set_name, self.labels.shape)\n",
    "\n",
    "        else:\n",
    "            f = self.test_list[0][0]\n",
    "            file = os.path.join(self.root, self.base_folder, f)\n",
    "            fo = open(file, 'rb')\n",
    "            if sys.version_info[0] == 2:\n",
    "                entry = pickle.load(fo)\n",
    "            else:\n",
    "                entry = pickle.load(fo, encoding='latin1')\n",
    "            self.data = entry['data']\n",
    "            if 'labels' in entry:\n",
    "                self.labels = entry['labels']\n",
    "            else:\n",
    "                self.labels = entry['fine_labels']\n",
    "            fo.close()\n",
    "            self.data = self.data.reshape((10000, 3, 32, 32))\n",
    "            self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "            self.labels = np.array(self.labels)\n",
    "            print(set_name, self.data.shape)\n",
    "            print(set_name, self.labels.shape)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.labels[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        root = self.root\n",
    "        for fentry in (self.train_list + self.test_list):\n",
    "            filename, md5 = fentry[0], fentry[1]\n",
    "            fpath = os.path.join(root, self.base_folder, filename)\n",
    "            if not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        root = self.root\n",
    "        download_url(self.url, root, self.filename, self.tgz_md5)\n",
    "\n",
    "        # extract file\n",
    "        cwd = os.getcwd()\n",
    "        tar = tarfile.open(os.path.join(root, self.filename), \"r:gz\")\n",
    "        os.chdir(root)\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        os.chdir(cwd)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = self.set_name\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "\n",
    "class CIFAR100(CIFAR10):\n",
    "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the `CIFAR10` Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-100-python'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:18:23.959053Z",
     "iopub.status.busy": "2021-12-03T15:18:23.958454Z",
     "iopub.status.idle": "2021-12-03T15:18:35.771932Z",
     "shell.execute_reply": "2021-12-03T15:18:35.771244Z",
     "shell.execute_reply.started": "2021-12-03T15:18:23.959010Z"
    }
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.RandomRotation(20),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_data = CIFAR100(root='data', set_name='train',\n",
    "                 transform=transform_train,\n",
    "                 download=True)  # initialize our rngs using the argument set seed\n",
    "val_data = CIFAR100(root='data', set_name='val',\n",
    "                 transform=transform_test,\n",
    "                 download=True)  # initialize our rngs using the argument set seed\n",
    "test_data = CIFAR100(root='data', set_name='test',\n",
    "                 transform=transform_test,\n",
    "                 download=True)  # initialize our rngs using the argument set seed\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=CFG.batch_size, shuffle=True, num_workers=4)\n",
    "val_data_loader = DataLoader(val_data, batch_size=CFG.batch_size, shuffle=True, num_workers=4)\n",
    "test_data_loader = DataLoader(test_data, batch_size=CFG.batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:18:42.845967Z",
     "iopub.status.busy": "2021-12-03T15:18:42.845689Z",
     "iopub.status.idle": "2021-12-03T15:18:42.908970Z",
     "shell.execute_reply": "2021-12-03T15:18:42.908244Z",
     "shell.execute_reply.started": "2021-12-03T15:18:42.845935Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExperimentBuilder(nn.Module):\n",
    "    def __init__(self, network_model, experiment_name, num_epochs, train_data, val_data,\n",
    "                 test_data, lr, weight_decay_coefficient, use_gpu, continue_from_epoch=-1):\n",
    "        \"\"\"\n",
    "        Initializes an ExperimentBuilder object. Such an object takes care of running training and evaluation of a deep net\n",
    "        on a given dataset. It also takes care of saving per epoch models and automatically inferring the best val model\n",
    "        to be used for evaluating the test set metrics.\n",
    "        :param network_model: A pytorch nn.Module which implements a network architecture.\n",
    "        :param experiment_name: The name of the experiment. This is used mainly for keeping track of the experiment and creating and directory structure that will be used to save logs, model parameters and other.\n",
    "        :param num_epochs: Total number of epochs to run the experiment\n",
    "        :param train_data: An object of the DataProvider type. Contains the training set.\n",
    "        :param val_data: An object of the DataProvider type. Contains the val set.\n",
    "        :param test_data: An object of the DataProvider type. Contains the test set.\n",
    "        :param weight_decay_coefficient: A float indicating the weight decay to use with the adam optimizer.\n",
    "        :param use_gpu: A boolean indicating whether to use a GPU or not.\n",
    "        :param continue_from_epoch: An int indicating whether we'll start from scrach (-1) or whether we'll reload a previously saved model of epoch 'continue_from_epoch' and continue training from there.\n",
    "        \"\"\"\n",
    "        super(ExperimentBuilder, self).__init__()\n",
    "\n",
    "\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model = network_model\n",
    "\n",
    "        if torch.cuda.device_count() > 1 and use_gpu:\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model.to(self.device)\n",
    "            self.model = nn.DataParallel(module=self.model)\n",
    "            print('Use Multi GPU', self.device)\n",
    "        elif torch.cuda.device_count() == 1 and use_gpu:\n",
    "            self.device =  torch.cuda.current_device()\n",
    "            self.model.to(self.device)  # sends the model from the cpu to the gpu\n",
    "            print('Use GPU', self.device)\n",
    "        else:\n",
    "            print(\"use CPU\")\n",
    "            self.device = torch.device('cpu')  # sets the device to be CPU\n",
    "            print(self.device)\n",
    "\n",
    "        print('here')\n",
    "\n",
    "        self.model.reset_parameters()  # re-initialize network parameters\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        print('System learnable parameters')\n",
    "        num_conv_layers = 0\n",
    "        num_linear_layers = 0\n",
    "        total_num_parameters = 0\n",
    "        for name, value in self.named_parameters():\n",
    "            print(name, value.shape)\n",
    "            if all(item in name for item in ['conv', 'weight']):\n",
    "                num_conv_layers += 1\n",
    "            if all(item in name for item in ['linear', 'weight']):\n",
    "                num_linear_layers += 1\n",
    "            total_num_parameters += np.prod(value.shape)\n",
    "\n",
    "        print('Total number of parameters', total_num_parameters)\n",
    "        print('Total number of conv layers', num_conv_layers)\n",
    "        print('Total number of linear layers', num_linear_layers)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), amsgrad=False, lr = lr,\n",
    "                                    weight_decay=weight_decay_coefficient)\n",
    "        self.learning_rate_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer,\n",
    "                                                                            T_max=num_epochs,\n",
    "                                                                            eta_min=0.00002)\n",
    "        # Generate the directory names\n",
    "        self.experiment_folder = os.path.abspath(experiment_name)\n",
    "        self.experiment_logs = os.path.abspath(os.path.join(self.experiment_folder, \"result_outputs\"))\n",
    "        self.experiment_saved_models = os.path.abspath(os.path.join(self.experiment_folder, \"saved_models\"))\n",
    "\n",
    "        # Set best models to be at 0 since we are just starting\n",
    "        self.best_val_model_idx = 0\n",
    "        self.best_val_model_acc = 0.\n",
    "\n",
    "        if not os.path.exists(self.experiment_folder):  # If experiment directory does not exist\n",
    "            os.mkdir(self.experiment_folder)  # create the experiment directory\n",
    "            os.mkdir(self.experiment_logs)  # create the experiment log directory\n",
    "            os.mkdir(self.experiment_saved_models)  # create the experiment saved models directory\n",
    "\n",
    "        self.num_epochs = num_epochs\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)  # send the loss computation to the GPU\n",
    "\n",
    "        if continue_from_epoch == -2:  # if continue from epoch is -2 then continue from latest saved model\n",
    "            self.state, self.best_val_model_idx, self.best_val_model_acc = self.load_model(\n",
    "                model_save_dir=self.experiment_saved_models, model_save_name=\"train_model\",\n",
    "                model_idx='latest')  # reload existing model from epoch and return best val model index\n",
    "            # and the best val acc of that model\n",
    "            self.starting_epoch = int(self.state['model_epoch'])\n",
    "\n",
    "        elif continue_from_epoch > -1:  # if continue from epoch is greater than -1 then\n",
    "            self.state, self.best_val_model_idx, self.best_val_model_acc = self.load_model(\n",
    "                model_save_dir=self.experiment_saved_models, model_save_name=\"train_model\",\n",
    "                model_idx=continue_from_epoch)  # reload existing model from epoch and return best val model index\n",
    "            # and the best val acc of that model\n",
    "            self.starting_epoch = continue_from_epoch\n",
    "        else:\n",
    "            self.state = dict()\n",
    "            self.starting_epoch = 0\n",
    "\n",
    "    def get_num_parameters(self):\n",
    "        total_num_params = 0\n",
    "        for param in self.parameters():\n",
    "            total_num_params += np.prod(param.shape)\n",
    "\n",
    "        return total_num_params\n",
    "\n",
    "\n",
    "    def plot_func_def(self,all_grads, layers):\n",
    "        \n",
    "       \n",
    "        \"\"\"\n",
    "        Plot function definition to plot the average gradient with respect to the number of layers in the given model\n",
    "        :param all_grads: Gradients wrt weights for each layer in the model.\n",
    "        :param layers: Layer names corresponding to the model parameters\n",
    "        :return: plot for gradient flow\n",
    "        \"\"\"\n",
    "        plt.plot(all_grads, alpha=0.3, color=\"b\")\n",
    "        plt.hlines(0, 0, len(all_grads)+1, linewidth=1, color=\"k\" )\n",
    "        plt.xticks(range(0,len(all_grads), 1), layers, rotation=\"vertical\")\n",
    "        plt.xlim(xmin=0, xmax=len(all_grads))\n",
    "        plt.xlabel(\"Layers\")\n",
    "        plt.ylabel(\"Average Gradient\")\n",
    "        plt.title(\"Gradient flow\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt\n",
    "        \n",
    "    \n",
    "    def plot_grad_flow(self, named_parameters):\n",
    "        \"\"\"\n",
    "        The function is being called in Line 298 of this file. \n",
    "        Receives the parameters of the model being trained. Returns plot of gradient flow for the given model parameters.\n",
    "       \n",
    "        \"\"\"\n",
    "        all_grads = []\n",
    "        layers = []\n",
    "        \n",
    "        \"\"\"\n",
    "        Complete the code in the block below to collect absolute mean of the gradients for each layer in all_grads with the             layer names in layers.\n",
    "        \"\"\"\n",
    "        ########################################\n",
    "        \n",
    "        for name,params in named_parameters:\n",
    "            if (params.requires_grad) and ('batch_norm' not in name) and ('bias' not in name):\n",
    "                all_grads.append(params.grad.abs().mean())\n",
    "                n = name.replace('layer_dict.','_')\n",
    "                layer_name = n.replace('.','')\n",
    "                if layer_name.startswith('_'):\n",
    "                    layer_name = layer_name[1:]\n",
    "                layers.append(layer_name.replace('weight',''))\n",
    "\n",
    "        ########################################\n",
    "            \n",
    "        \n",
    "        plt = self.plot_func_def(all_grads, layers)\n",
    "        \n",
    "        return plt\n",
    "\n",
    "\n",
    "    def plot_grad_flow_with_bn(self, named_parameters):\n",
    "        \"\"\"\n",
    "        The function is being called in Line 298 of this file. \n",
    "        Receives the parameters of the model being trained. Returns plot of gradient flow for the given model parameters.\n",
    "       \n",
    "        \"\"\"\n",
    "        all_grads = []\n",
    "        layers = []\n",
    "        \n",
    "        \"\"\"\n",
    "        Complete the code in the block below to collect absolute mean of the gradients for each layer in all_grads with the             layer names in layers.\n",
    "        \"\"\"\n",
    "        ########################################\n",
    "        \n",
    "        for name,params in named_parameters:\n",
    "            if (params.requires_grad) and ('bias' not in name):\n",
    "                all_grads.append(params.grad.abs().mean())\n",
    "                n = name.replace('layer_dict.','_')[1:]\n",
    "                layer_name = n.replace('.','')\n",
    "                if layer_name.startswith('_'):\n",
    "                    layer_name = layer_name[1:]\n",
    "                layers.append(layer_name.replace('weight',''))\n",
    "\n",
    "        ########################################\n",
    "\n",
    "        plt.plot(all_grads, alpha=0.3, color=\"b\")\n",
    "        plt.hlines(0, 0, len(all_grads)+1, linewidth=1, color=\"k\" )\n",
    "        #plt.xticks(range(0,len(all_grads), 1), layers, rotation=\"vertical\")\n",
    "        plt.xlim(xmin=0, xmax=len(all_grads))\n",
    "        plt.xlabel(\"Layers\")\n",
    "        plt.ylabel(\"Average Gradient\")\n",
    "        plt.title(\"Gradient flow\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "            \n",
    "        \n",
    "        #plt = self.plot_func_def(all_grads, layers)\n",
    "        \n",
    "        return plt\n",
    "    \n",
    "    def run_train_iter(self, x, y):\n",
    "        \n",
    "        self.train()  # sets model to training mode (in case batch normalization or other methods have different procedures for training and evaluation)\n",
    "        x, y = x.float().to(device=self.device), y.long().to(\n",
    "            device=self.device)  # send data to device as torch tensors\n",
    "        out = self.model.forward(x)  # forward the data in the model\n",
    "\n",
    "\n",
    "        loss = F.cross_entropy(input=out, target=y)  # compute loss\n",
    "\n",
    "        self.optimizer.zero_grad()  # set all weight grads from previous training iters to 0\n",
    "        loss.backward()  # backpropagate to compute gradients for current iter loss\n",
    "        \n",
    "        self.learning_rate_scheduler.step(epoch=self.current_epoch)\n",
    "        self.optimizer.step()  # update network parameters\n",
    "        _, predicted = torch.max(out.data, 1)  # get argmax of predictions\n",
    "        accuracy = np.mean(list(predicted.eq(y.data).cpu()))  # compute accuracy\n",
    "        return loss.cpu().data.numpy(), accuracy\n",
    "\n",
    "    def run_evaluation_iter(self, x, y):\n",
    "        \"\"\"\n",
    "        Receives the inputs and targets for the model and runs an evaluation iterations. Returns loss and accuracy metrics.\n",
    "        :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n",
    "        :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n",
    "        :return: the loss and accuracy for this batch\n",
    "        \"\"\"\n",
    "        self.eval()  # sets the system to validation mode\n",
    "        x, y = x.float().to(device=self.device), y.long().to(\n",
    "            device=self.device)  # convert data to pytorch tensors and send to the computation device\n",
    "        out = self.model.forward(x)  # forward the data in the model\n",
    "\n",
    "        loss = F.cross_entropy(input=out, target=y)  # compute loss\n",
    "\n",
    "        _, predicted = torch.max(out.data, 1)  # get argmax of predictions\n",
    "        accuracy = np.mean(list(predicted.eq(y.data).cpu()))  # compute accuracy\n",
    "        return loss.cpu().data.numpy(), accuracy\n",
    "\n",
    "    def save_model(self, model_save_dir, model_save_name, model_idx, best_validation_model_idx,\n",
    "                   best_validation_model_acc):\n",
    "        \"\"\"\n",
    "        Save the network parameter state and current best val epoch idx and best val accuracy.\n",
    "        :param model_save_name: Name to use to save model without the epoch index\n",
    "        :param model_idx: The index to save the model with.\n",
    "        :param best_validation_model_idx: The index of the best validation model to be stored for future use.\n",
    "        :param best_validation_model_acc: The best validation accuracy to be stored for use at test time.\n",
    "        :param model_save_dir: The directory to store the state at.\n",
    "        :param state: The dictionary containing the system state.\n",
    "        \"\"\"\n",
    "        self.state['network'] = self.state_dict()  # save network parameter and other variables.\n",
    "        self.state['best_val_model_idx'] = best_validation_model_idx  # save current best val idx\n",
    "        self.state['best_val_model_acc'] = best_validation_model_acc  # save current best val acc\n",
    "        torch.save(self.state, f=os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(\n",
    "            model_idx))))  # save state at prespecified filepath\n",
    "\n",
    "    def load_model(self, model_save_dir, model_save_name, model_idx):\n",
    "        \"\"\"\n",
    "        Load the network parameter state and the best val model idx and best val acc to be compared with the future val accuracies, in order to choose the best val model\n",
    "        :param model_save_dir: The directory to store the state at.\n",
    "        :param model_save_name: Name to use to save model without the epoch index\n",
    "        :param model_idx: The index to save the model with.\n",
    "        :return: best val idx and best val model acc, also it loads the network state into the system state without returning it\n",
    "        \"\"\"\n",
    "        state = torch.load(f=os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(model_idx))))\n",
    "        self.load_state_dict(state_dict=state['network'])\n",
    "        return state, state['best_val_model_idx'], state['best_val_model_acc']\n",
    "\n",
    "    def run_experiment(self):\n",
    "        \"\"\"\n",
    "        Runs experiment train and evaluation iterations, saving the model and best val model and val model accuracy after each epoch\n",
    "        :return: The summary current_epoch_losses from starting epoch to total_epochs.\n",
    "        \"\"\"\n",
    "        total_losses = {\"train_acc\": [], \"train_loss\": [], \"val_acc\": [],\n",
    "                        \"val_loss\": []}  # initialize a dict to keep the per-epoch metrics\n",
    "        for i, epoch_idx in enumerate(range(self.starting_epoch, self.num_epochs)):\n",
    "            epoch_start_time = time.time()\n",
    "            current_epoch_losses = {\"train_acc\": [], \"train_loss\": [], \"val_acc\": [], \"val_loss\": []}\n",
    "            self.current_epoch = epoch_idx\n",
    "            with tqdm.tqdm(total=len(self.train_data)) as pbar_train:  # create a progress bar for training\n",
    "                for idx, (x, y) in enumerate(self.train_data):  # get data batches\n",
    "                    loss, accuracy = self.run_train_iter(x=x, y=y)  # take a training iter step\n",
    "                    current_epoch_losses[\"train_loss\"].append(loss)  # add current iter loss to the train loss list\n",
    "                    current_epoch_losses[\"train_acc\"].append(accuracy)  # add current iter acc to the train acc list\n",
    "                    pbar_train.update(1)\n",
    "                    pbar_train.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(loss, accuracy))\n",
    "\n",
    "            with tqdm.tqdm(total=len(self.val_data)) as pbar_val:  # create a progress bar for validation\n",
    "                for x, y in self.val_data:  # get data batches\n",
    "                    loss, accuracy = self.run_evaluation_iter(x=x, y=y)  # run a validation iter\n",
    "                    current_epoch_losses[\"val_loss\"].append(loss)  # add current iter loss to val loss list.\n",
    "                    current_epoch_losses[\"val_acc\"].append(accuracy)  # add current iter acc to val acc lst.\n",
    "                    pbar_val.update(1)  # add 1 step to the progress bar\n",
    "                    pbar_val.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(loss, accuracy))\n",
    "            val_mean_accuracy = np.mean(current_epoch_losses['val_acc'])\n",
    "            if val_mean_accuracy > self.best_val_model_acc:  # if current epoch's mean val acc is greater than the saved best val acc then\n",
    "                self.best_val_model_acc = val_mean_accuracy  # set the best val model acc to be current epoch's val accuracy\n",
    "                self.best_val_model_idx = epoch_idx  # set the experiment-wise best val idx to be the current epoch's idx\n",
    "\n",
    "            for key, value in current_epoch_losses.items():\n",
    "                total_losses[key].append(np.mean(\n",
    "                    value))  # get mean of all metrics of current epoch metrics dict, to get them ready for storage and output on the terminal.\n",
    "\n",
    "            save_statistics(experiment_log_dir=self.experiment_logs, filename='summary.csv',\n",
    "                            stats_dict=total_losses, current_epoch=i,\n",
    "                            continue_from_mode=True if (self.starting_epoch != 0 or i > 0) else False)  # save statistics to stats file.\n",
    "\n",
    "            # load_statistics(experiment_log_dir=self.experiment_logs, filename='summary.csv') # How to load a csv file if you need to\n",
    "\n",
    "            out_string = \"_\".join(\n",
    "                [\"{}_{:.4f}\".format(key, np.mean(value)) for key, value in current_epoch_losses.items()])\n",
    "            # create a string to use to report our epoch metrics\n",
    "            epoch_elapsed_time = time.time() - epoch_start_time  # calculate time taken for epoch\n",
    "            epoch_elapsed_time = \"{:.4f}\".format(epoch_elapsed_time)\n",
    "            print(\"Epoch {}:\".format(epoch_idx), out_string, \"epoch time\", epoch_elapsed_time, \"seconds\")\n",
    "            self.state['model_epoch'] = epoch_idx\n",
    "            self.save_model(model_save_dir=self.experiment_saved_models,\n",
    "                            # save model and best val idx and best val acc, using the model dir, model name and model idx\n",
    "                            model_save_name=\"train_model\", model_idx=epoch_idx,\n",
    "                            best_validation_model_idx=self.best_val_model_idx,\n",
    "                            best_validation_model_acc=self.best_val_model_acc)\n",
    "            self.save_model(model_save_dir=self.experiment_saved_models,\n",
    "                            # save model and best val idx and best val acc, using the model dir, model name and model idx\n",
    "                            model_save_name=\"train_model\", model_idx='latest',\n",
    "                            best_validation_model_idx=self.best_val_model_idx,\n",
    "                            best_validation_model_acc=self.best_val_model_acc)\n",
    "            \n",
    "            ################################################################\n",
    "            ##### Plot Gradient Flow at each Epoch during Training  ######\n",
    "            print(\"Generating Gradient Flow Plot at epoch {}\".format(epoch_idx))\n",
    "            plt = self.plot_grad_flow(self.model.named_parameters())\n",
    "            #plt_with_bn = self.plot_grad_flow_with_bn(self.model.named_parameters())\n",
    "            if not os.path.exists(os.path.join(self.experiment_saved_models, 'gradient_flow_plots')):\n",
    "                os.mkdir(os.path.join(self.experiment_saved_models, 'gradient_flow_plots'))\n",
    "                # plt.legend(loc=\"best\")\n",
    "            plt.savefig(os.path.join(self.experiment_saved_models, 'gradient_flow_plots', \"epoch{}.pdf\".format(str(epoch_idx))))\n",
    "            #if not os.path.exists(os.path.join(self.experiment_saved_models, 'gradient_flow_plots_with_bn')):\n",
    "                #os.mkdir(os.path.join(self.experiment_saved_models, 'gradient_flow_plots_with_bn'))\n",
    "            #plt_with_bn.savefig(os.path.join(self.experiment_saved_models,'gradient_flow_plots_with_bn',f\"epoch{epoch_idx}.pdf\"))\n",
    "            ################################################################\n",
    "        \n",
    "        print(\"Generating test set evaluation metrics\")\n",
    "        self.load_model(model_save_dir=self.experiment_saved_models, model_idx=self.best_val_model_idx,\n",
    "                        # load best validation model\n",
    "                        model_save_name=\"train_model\")\n",
    "        current_epoch_losses = {\"test_acc\": [], \"test_loss\": []}  # initialize a statistics dict\n",
    "        with tqdm.tqdm(total=len(self.test_data)) as pbar_test:  # ini a progress bar\n",
    "            for x, y in self.test_data:  # sample batch\n",
    "                loss, accuracy = self.run_evaluation_iter(x=x,\n",
    "                                                          y=y)  # compute loss and accuracy by running an evaluation step\n",
    "                current_epoch_losses[\"test_loss\"].append(loss)  # save test loss\n",
    "                current_epoch_losses[\"test_acc\"].append(accuracy)  # save test accuracy\n",
    "                pbar_test.update(1)  # update progress bar status\n",
    "                pbar_test.set_description(\n",
    "                    \"loss: {:.4f}, accuracy: {:.4f}\".format(loss, accuracy))  # update progress bar string output\n",
    "\n",
    "        test_losses = {key: [np.mean(value)] for key, value in\n",
    "                       current_epoch_losses.items()}  # save test set metrics in dict format\n",
    "        save_statistics(experiment_log_dir=self.experiment_logs, filename='test_summary.csv',\n",
    "                        # save test set metrics on disk in .csv format\n",
    "                        stats_dict=test_losses, current_epoch=0, continue_from_mode=False)\n",
    "\n",
    "        return total_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:18:55.722041Z",
     "iopub.status.busy": "2021-12-03T15:18:55.721792Z",
     "iopub.status.idle": "2021-12-03T15:18:55.814054Z",
     "shell.execute_reply": "2021-12-03T15:18:55.813286Z",
     "shell.execute_reply.started": "2021-12-03T15:18:55.722011Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FCCNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_output_classes, num_filters, num_layers, use_bias=False):\n",
    "        \"\"\"\n",
    "        Initializes a fully connected network similar to the ones implemented previously in the MLP package.\n",
    "        :param input_shape: The shape of the inputs going in to the network.\n",
    "        :param num_output_classes: The number of outputs the network should have (for classification those would be the number of classes)\n",
    "        :param num_filters: Number of filters used in every fcc layer.\n",
    "        :param num_layers: Number of fcc layers (excluding dim reduction stages)\n",
    "        :param use_bias: Whether our fcc layers will use a bias.\n",
    "        \"\"\"\n",
    "        super(FCCNetwork, self).__init__()\n",
    "        # set up class attributes useful in building the network and inference\n",
    "        self.input_shape = input_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.num_output_classes = num_output_classes\n",
    "        self.use_bias = use_bias\n",
    "        self.num_layers = num_layers\n",
    "        # initialize a module dict, which is effectively a dictionary that can collect layers and integrate them into pytorch\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        # build the network\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        print(\"Building basic block of FCCNetwork using input shape\", self.input_shape)\n",
    "        x = torch.zeros((self.input_shape))\n",
    "\n",
    "        out = x\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        # flatten inputs to shape (b, -1) where -1 is the dim resulting from multiplying the\n",
    "        # shapes of all dimensions after the 0th dim\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.layer_dict['fcc_{}'.format(i)] = nn.Linear(in_features=out.shape[1],  # initialize a fcc layer\n",
    "                                                            out_features=self.num_filters,\n",
    "                                                            bias=self.use_bias)\n",
    "\n",
    "            out = self.layer_dict['fcc_{}'.format(i)](out)  # apply ith fcc layer to the previous layers outputs\n",
    "            out = F.relu(out)  # apply a ReLU on the outputs\n",
    "\n",
    "        self.logits_linear_layer = nn.Linear(in_features=out.shape[1],  # initialize the prediction output linear layer\n",
    "                                             out_features=self.num_output_classes,\n",
    "                                             bias=self.use_bias)\n",
    "        out = self.logits_linear_layer(out)  # apply the layer to the previous layer's outputs\n",
    "        print(\"Block is built, output volume is\", out.shape)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward prop data through the network and return the preds\n",
    "        :param x: Input batch x a batch of shape batch number of samples, each of any dimensionality.\n",
    "        :return: preds of shape (b, num_classes)\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        # flatten inputs to shape (b, -1) where -1 is the dim resulting from multiplying the\n",
    "        # shapes of all dimensions after the 0th dim\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            out = self.layer_dict['fcc_{}'.format(i)](out)  # apply ith fcc layer to the previous layers outputs\n",
    "            out = F.relu(out)  # apply a ReLU on the outputs\n",
    "\n",
    "        out = self.logits_linear_layer(out)  # apply the layer to the previous layer's outputs\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Re-initializes the networks parameters\n",
    "        \"\"\"\n",
    "        for item in self.layer_dict.children():\n",
    "            item.reset_parameters()\n",
    "\n",
    "        self.logits_linear_layer.reset_parameters()\n",
    "\n",
    "\n",
    "class EmptyBlock(nn.Module):\n",
    "    def __init__(self, input_shape=None, num_filters=None, kernel_size=None, padding=None, bias=None, dilation=None,\n",
    "                 reduction_factor=None):\n",
    "        super(EmptyBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        self.layer_dict['Identity'] = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['Identity'].forward(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EntryConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation):\n",
    "        super(EntryConvolutionalBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        self.layer_dict['bn_0'] = nn.BatchNorm2d(num_features=out.shape[1])\n",
    "        out = F.leaky_relu(self.layer_dict['bn_0'].forward(out))\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(self.layer_dict['bn_0'].forward(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "class ConvolutionalProcessingBlock(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation):\n",
    "        super(ConvolutionalProcessingBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvolutionalDimensionalityReductionBlock(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation, reduction_factor):\n",
    "        super(ConvolutionalDimensionalityReductionBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ConvolutionalProcessingBlockBNRC(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation):\n",
    "        super(ConvolutionalProcessingBlockBNRC, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        identity = x\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        \n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        self.layer_dict['batch_norm_0'] = nn.BatchNorm2d(num_features=out.shape[1])\n",
    "\n",
    "        \n",
    "        out = self.layer_dict['batch_norm_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        \n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "\n",
    "        self.layer_dict['batch_norm_1'] = nn.BatchNorm2d(num_features=out.shape[1])\n",
    "        out = self.layer_dict['batch_norm_1'].forward(out)\n",
    "\n",
    "        out += identity\n",
    "\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = self.layer_dict['batch_norm_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = self.layer_dict['batch_norm_1'].forward(out)\n",
    "\n",
    "        out += identity\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ConvProcessingBlockBN(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation):\n",
    "        super(ConvProcessingBlockBN, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        self.layer_dict['batch_norm_0'] = nn.BatchNorm2d(self.num_filters)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = self.layer_dict['batch_norm_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        self.layer_dict['batch_norm_1'] = nn.BatchNorm2d(self.num_filters)\n",
    "                                  \n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = self.layer_dict['batch_norm_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = self.layer_dict['batch_norm_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = self.layer_dict['batch_norm_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ConvDimReductionBlockBN(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation, reduction_factor):\n",
    "        super(ConvDimReductionBlockBN, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        self.layer_dict['batch_norm_0'] = nn.BatchNorm2d(self.num_filters)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        #out = self.layer_dict['batch_norm_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "        out = self.layer_dict['batch_norm_0'].forward(out)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "        self.layer_dict['batch_norm_1'] = nn.BatchNorm2d(self.num_filters)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = self.layer_dict['batch_norm_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "        out = self.layer_dict['batch_norm_0'].forward(out)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = self.layer_dict['batch_norm_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_output_classes, num_filters,\n",
    "                 num_blocks_per_stage, num_stages, use_bias=False, processing_block_type=ConvolutionalProcessingBlockBNRC,\n",
    "                 dimensionality_reduction_block_type=ConvolutionalDimensionalityReductionBlockBNRC):\n",
    "        \"\"\"\n",
    "        Initializes a convolutional network module\n",
    "        :param input_shape: The shape of the tensor to be passed into this network\n",
    "        :param num_output_classes: Number of output classes\n",
    "        :param num_filters: Number of filters per convolutional layer\n",
    "        :param num_blocks_per_stage: Number of blocks per \"stage\". Each block is composed of 2 convolutional layers.\n",
    "        :param num_stages: Number of stages in a network. A stage is defined as a sequence of layers within which the\n",
    "        data dimensionality remains constant in the spacial axis (h, w) and can change in the channel axis. After each stage\n",
    "        there exists a dimensionality reduction stage, composed of two convolutional layers and an avg pooling layer.\n",
    "        :param use_bias: Whether to use biases in our convolutional layers\n",
    "        :param processing_block_type: Type of processing block to use within our stages\n",
    "        :param dimensionality_reduction_block_type: Type of dimensionality reduction block to use after each stage in our network\n",
    "        \"\"\"\n",
    "        super(ConvolutionalNetwork, self).__init__()\n",
    "        # set up class attributes useful in building the network and inference\n",
    "        self.input_shape = input_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.num_output_classes = num_output_classes\n",
    "        self.use_bias = use_bias\n",
    "        self.num_blocks_per_stage = num_blocks_per_stage\n",
    "        self.num_stages = num_stages\n",
    "        self.processing_block_type = processing_block_type\n",
    "        self.dimensionality_reduction_block_type = dimensionality_reduction_block_type\n",
    "\n",
    "        # build the network\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        \"\"\"\n",
    "        Builds network whilst automatically inferring shapes of layers.\n",
    "        \"\"\"\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        # initialize a module dict, which is effectively a dictionary that can collect layers and integrate them into pytorch\n",
    "        print(\"Building basic block of ConvolutionalNetwork using input shape\", self.input_shape)\n",
    "        x = torch.zeros((self.input_shape))  # create dummy inputs to be used to infer shapes of layers\n",
    "\n",
    "        out = x\n",
    "        self.layer_dict['input_conv'] = EntryConvolutionalBlock(input_shape=out.shape, num_filters=self.num_filters,\n",
    "                                                                kernel_size=3, padding=1, bias=self.use_bias,\n",
    "                                                                dilation=1)\n",
    "        out = self.layer_dict['input_conv'].forward(out)\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        for i in range(self.num_stages):  # for number of layers times\n",
    "            for j in range(self.num_blocks_per_stage):\n",
    "                self.layer_dict['block_{}_{}'.format(i, j)] = self.processing_block_type(input_shape=out.shape,\n",
    "                                                                                         num_filters=self.num_filters,\n",
    "                                                                                         bias=self.use_bias,\n",
    "                                                                                         kernel_size=3, dilation=1,\n",
    "                                                                                         padding=1)\n",
    "                out = self.layer_dict['block_{}_{}'.format(i, j)].forward(out)\n",
    "            self.layer_dict['reduction_block_{}'.format(i)] = self.dimensionality_reduction_block_type(\n",
    "                input_shape=out.shape,\n",
    "                num_filters=self.num_filters, bias=True,\n",
    "                kernel_size=3, dilation=1,\n",
    "                padding=1,\n",
    "                reduction_factor=2)\n",
    "            out = self.layer_dict['reduction_block_{}'.format(i)].forward(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, out.shape[-1])\n",
    "        print('shape before final linear layer', out.shape)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        self.logit_linear_layer = nn.Linear(in_features=out.shape[1],  # add a linear layer\n",
    "                                            out_features=self.num_output_classes,\n",
    "                                            bias=True)\n",
    "        out = self.logit_linear_layer(out)  # apply linear layer on flattened inputs\n",
    "        print(\"Block is built, output volume is\", out.shape)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propages the network given an input batch\n",
    "        :param x: Inputs x (b, c, h, w)\n",
    "        :return: preds (b, num_classes)\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        out = self.layer_dict['input_conv'].forward(out)\n",
    "        for i in range(self.num_stages):  # for number of layers times\n",
    "            for j in range(self.num_blocks_per_stage):\n",
    "                out = self.layer_dict['block_{}_{}'.format(i, j)].forward(out)\n",
    "            out = self.layer_dict['reduction_block_{}'.format(i)].forward(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, out.shape[-1])\n",
    "        out = out.view(out.shape[0], -1)  # flatten outputs from (b, c, h, w) to (b, c*h*w)\n",
    "        out = self.logit_linear_layer(out)  # pass through a linear layer to get logits/preds\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Re-initialize the network parameters.\n",
    "        \"\"\"\n",
    "        for item in self.layer_dict.children():\n",
    "            try:\n",
    "                item.reset_parameters()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        self.logit_linear_layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_08 = ConvolutionalNetwork(  # initialize our network object, in this case a ConvNet\n",
    "    input_shape=CFG.input_shape,\n",
    "    num_output_classes=CFG.num_classes, num_filters=CFG.num_filters, use_bias=False,\n",
    "    num_blocks_per_stage=0, num_stages=3,\n",
    "    processing_block_type=ConvolutionalProcessingBlock,\n",
    "    dimensionality_reduction_block_type= ConvolutionalDimensionalityReductionBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:19:03.666681Z",
     "iopub.status.busy": "2021-12-03T15:19:03.666236Z",
     "iopub.status.idle": "2021-12-03T15:19:05.016893Z",
     "shell.execute_reply": "2021-12-03T15:19:05.015769Z",
     "shell.execute_reply.started": "2021-12-03T15:19:03.666644Z"
    }
   },
   "outputs": [],
   "source": [
    "vanilla_vgg_38 = ConvolutionalNetwork(  # initialize our network object, in this case a ConvNet\n",
    "    input_shape=CFG.input_shape,\n",
    "    num_output_classes=CFG.num_classes, num_filters=CFG.num_filters, use_bias=False,\n",
    "    num_blocks_per_stage=CFG.num_blocks_per_stage, num_stages=CFG.num_stages,\n",
    "    processing_block_type=ConvolutionalProcessingBlock,\n",
    "    dimensionality_reduction_block_type= ConvolutionalDimensionalityReductionBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:19:10.203216Z",
     "iopub.status.busy": "2021-12-03T15:19:10.202683Z",
     "iopub.status.idle": "2021-12-03T15:19:11.588439Z",
     "shell.execute_reply": "2021-12-03T15:19:11.587713Z",
     "shell.execute_reply.started": "2021-12-03T15:19:10.203177Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_38_bn = ConvolutionalNetwork(  # initialize our network object, in this case a ConvNet\n",
    "    input_shape=CFG.input_shape,\n",
    "    num_output_classes=CFG.num_classes, num_filters=CFG.num_filters, use_bias=False,\n",
    "    num_blocks_per_stage=CFG.num_blocks_per_stage, num_stages=CFG.num_stages,\n",
    "    processing_block_type=ConvProcessingBlockBN,\n",
    "    dimensionality_reduction_block_type= ConvDimReductionBlockBN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:19:27.369852Z",
     "iopub.status.busy": "2021-12-03T15:19:27.369315Z",
     "iopub.status.idle": "2021-12-03T15:19:28.653956Z",
     "shell.execute_reply": "2021-12-03T15:19:28.653244Z",
     "shell.execute_reply.started": "2021-12-03T15:19:27.369813Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_38_bn_rc = ConvolutionalNetwork(  # initialize our network object, in this case a ConvNet\n",
    "    input_shape=CFG.input_shape,\n",
    "    num_output_classes=CFG.num_classes, num_filters=CFG.num_filters, use_bias=False,\n",
    "    num_blocks_per_stage=CFG.num_blocks_per_stage, num_stages=CFG.num_stages,\n",
    "    processing_block_type=ConvolutionalProcessingBlockBNRC,\n",
    "    dimensionality_reduction_block_type= ConvDimReductionBlockBN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:23:07.353062Z",
     "iopub.status.busy": "2021-12-03T15:23:07.352442Z",
     "iopub.status.idle": "2021-12-03T15:23:10.001123Z",
     "shell.execute_reply": "2021-12-03T15:23:10.000332Z",
     "shell.execute_reply.started": "2021-12-03T15:23:07.353023Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer_bn_rc = ExperimentBuilder(vgg_38_bn_rc, './experiments/bnrc', CFG.num_epochs, train_data_loader, \n",
    "                                  val_data_loader,test_data_loader,CFG.lr_bn_rc, CFG.weight_decay, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:23:22.521378Z",
     "iopub.status.busy": "2021-12-03T15:23:22.520564Z",
     "iopub.status.idle": "2021-12-03T15:32:41.320966Z",
     "shell.execute_reply": "2021-12-03T15:32:41.319610Z",
     "shell.execute_reply.started": "2021-12-03T15:23:22.521334Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_metrics, test_losses = trainer_bn_rc.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:32:47.997516Z",
     "iopub.status.busy": "2021-12-03T15:32:47.997155Z",
     "iopub.status.idle": "2021-12-03T15:32:48.056698Z",
     "shell.execute_reply": "2021-12-03T15:32:48.056011Z",
     "shell.execute_reply.started": "2021-12-03T15:32:47.997474Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer_bn = ExperimentBuilder(vgg_38_bn, './experiments/bn', CFG.num_epochs, train_data_loader, \n",
    "                               val_data_loader,test_data_loader, CFG.lr_bn, CFG.weight_decay, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T15:32:52.483072Z",
     "iopub.status.busy": "2021-12-03T15:32:52.482487Z",
     "iopub.status.idle": "2021-12-03T16:01:35.520998Z",
     "shell.execute_reply": "2021-12-03T16:01:35.519702Z",
     "shell.execute_reply.started": "2021-12-03T15:32:52.483032Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_metrics_bn,test_losses_bn = trainer_bn.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_vanilla_vgg = ExperimentBuilder(vanilla_vgg_38, './experiments/vanilla', 100, train_data_loader, \n",
    "                               val_data_loader,test_data_loader, CFG.lr_bn, CFG.weight_decay, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_metrics_vanilla_vgg_38, test_losses_vanilla_vgg = trainer_vanilla_vgg.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T13:29:27.515141Z",
     "iopub.status.busy": "2021-12-03T13:29:27.514864Z",
     "iopub.status.idle": "2021-12-03T13:29:27.523693Z",
     "shell.execute_reply": "2021-12-03T13:29:27.522832Z",
     "shell.execute_reply.started": "2021-12-03T13:29:27.515111Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_exp_dicts(filepath):\n",
    "    experiment_dicts = dict()\n",
    "    with open(filepath, 'r') as read_file:\n",
    "        lines = read_file.readlines()\n",
    "                    \n",
    "        current_experiment_dict = {key: [] for key in lines[0].replace('\\n', '').split(',')}\n",
    "        idx_to_key = {idx: key for idx, key in enumerate(lines[0].replace('\\n', '').split(','))}\n",
    "                \n",
    "        for line in lines[1:]:\n",
    "            for idx, value in enumerate(line.replace('\\n', '').split(',')):\n",
    "                current_experiment_dict[idx_to_key[idx]].append(float(value))\n",
    "        experiment_dicts['VGG_38'] = current_experiment_dict\n",
    "                \n",
    "    return experiment_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T13:29:29.83996Z",
     "iopub.status.busy": "2021-12-03T13:29:29.839315Z",
     "iopub.status.idle": "2021-12-03T13:29:29.846265Z",
     "shell.execute_reply": "2021-12-03T13:29:29.845286Z",
     "shell.execute_reply.started": "2021-12-03T13:29:29.839894Z"
    }
   },
   "outputs": [],
   "source": [
    "result_dict_bn_rc = collect_exp_dicts('./experiments/bnrc/result_outputs/summary.csv')\n",
    "result_dict_bn = collect_exp_dicts('./experiments/bn/result_outputs/summary.csv')\n",
    "result_dict_vanilla = collect_exp_dicts('./experiments/vanilla/result_outputs/summary.csv')\n",
    "for key, value in result_dict.items():\n",
    "    print(key, list(value.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T13:29:32.119512Z",
     "iopub.status.busy": "2021-12-03T13:29:32.118713Z",
     "iopub.status.idle": "2021-12-03T13:29:32.129859Z",
     "shell.execute_reply": "2021-12-03T13:29:32.129133Z",
     "shell.execute_reply.started": "2021-12-03T13:29:32.119473Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_result_graphs(plot_name, stats, keys_to_plot):\n",
    "    \n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    #for name in keys_to_plot:\n",
    "    for k in ['train_loss', 'val_loss']:\n",
    "        item = stats[keys_to_plot][k]\n",
    "        ax_1.plot(np.arange(0, len(item)), \n",
    "                      item, label='{}_{}'.format(keys_to_plot, k))\n",
    "            \n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_ylabel('Loss')\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    #for name in keys_to_plot:\n",
    "    for k in ['train_acc', 'val_acc']:\n",
    "        item = stats[keys_to_plot][k]\n",
    "        ax_2.plot(np.arange(0, len(item)), \n",
    "                      item, label='{}_{}'.format(keys_to_plot, k))\n",
    "            \n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_ylabel('Accuracy')\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    fig_1.savefig('{}_loss_performance.pdf'.format(plot_name), dpi=None, facecolor='w', edgecolor='w',\n",
    "        orientation='portrait', papertype=None, format='pdf',\n",
    "        transparent=False, bbox_inches=None, pad_inches=0.1,\n",
    "        frameon=None, metadata=None)\n",
    "    \n",
    "    fig_2.savefig('{}_accuracy_performance.pdf'.format(plot_name), dpi=None, facecolor='w', edgecolor='w',\n",
    "        orientation='portrait', papertype=None, format='pdf',\n",
    "        transparent=False, bbox_inches=None, pad_inches=0.1,\n",
    "        frameon=None, metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T13:29:33.817789Z",
     "iopub.status.busy": "2021-12-03T13:29:33.817507Z",
     "iopub.status.idle": "2021-12-03T13:29:34.487302Z",
     "shell.execute_reply": "2021-12-03T13:29:34.486619Z",
     "shell.execute_reply.started": "2021-12-03T13:29:33.817752Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_result_graphs('problem_model_bn_rc', result_dict, keys_to_plot='VGG_38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result_graphs('problem_model_bn', result_dict, keys_to_plot='VGG_38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T13:38:13.144795Z",
     "iopub.status.busy": "2021-12-03T13:38:13.144471Z",
     "iopub.status.idle": "2021-12-03T13:38:13.164502Z",
     "shell.execute_reply": "2021-12-03T13:38:13.163699Z",
     "shell.execute_reply.started": "2021-12-03T13:38:13.144747Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLinks('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
